{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (366 kB)\n",
      "\u001b[K     |████████████████████████████████| 366 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (3.1.3)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (45.2.0.post20200210)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk\n",
    "!pip install missingno\n",
    "\n",
    "# for data\n",
    "import pandas as pd\n",
    "\n",
    "# for missing values\n",
    "import missingno as msno\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for word frequency\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spend_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General observations\n",
    "We have mostly categorical data in our dataset apart from the 'value' attribute, which is re-scaled by min-max normalization. (The 'Unnamed' column is not a feature, it contains the row indices). The text field attribute contains unstructured text data. Missing attribute values: in case of columns 1, 5 and 6, more than 99% of the data is missing -> I dropped these attributes. I dropped further attributes based on domain knowledge as they are not (always) available before classification.\n",
    "The remaining missing values in 'division' attribute are proven to be MAR (Missing at Random -> likelihood of missing is related to some of the observed data -> in our case profit_center). MAR is safe to remove the missing data without producing bias in the model and we have a large enough dataset to do that so we dropped the related rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of the data\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    411688.000000\n",
       "mean          0.000180\n",
       "std           0.003071\n",
       "min           0.000000\n",
       "25%           0.000004\n",
       "50%           0.000009\n",
       "75%           0.000033\n",
       "max           1.000000\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data from the value coloumn -> normalized by min-max\n",
    "\n",
    "df['value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate missing values statistics\n",
    "\n",
    "def null_values(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "    \n",
    "miss_values = null_values(df)\n",
    "miss_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411688, 19)\n"
     ]
    }
   ],
   "source": [
    "# drop columns based on missing values statistics -  > 99% of missing values\n",
    "\n",
    "df = df.drop(columns=['col1', 'col5', 'col6'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns based on domain knowledge -> , \n",
    "# subcategory, GL account data are not (always) known prior the classification\n",
    "\n",
    "df = df.drop(columns=['col7','col8','col9','col10','col11','col12','col13','col14','col15'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing values in 'division' -> MAR, MCAR or MNAR?\n",
    "\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAR -> The likelihood of being missing depends on profit_center data -> it is safe to remove the missing data and we have a large enough\n",
    "# dataset to do that\n",
    "\n",
    "msno.matrix(df.sort_values('col2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample questions\n",
    "\n",
    "### Class distribution\n",
    "We clearly have an imbalanced class problem here where the number of observations per class are not equally distributed. E.g., the number of purchase item requests belonging to \"office supplies\" is more than 8 650 times of \"mass storage\" items. As conventional algorithms are often biased towards the majority class(es) this can lead to low accuracy in minority classes. In our case the value distribution adds further complexity as not the most populated classes account for the highest value. (The domain-experts expressed the wish for the higher value purchase requests to be classified more accuratly.)\n",
    "There are several approaches to handle the imbalance class problem like cost function based approaches and sampling based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sample question: How many items are in which class? -> imbalanced classes\n",
    "\n",
    "# item count by classes\n",
    "print(df['group'].value_counts())\n",
    "\n",
    "# plot it as a histogram\n",
    "print(df['group'].value_counts().sort_values().plot(kind='barh',colormap='Paired',\n",
    "                                                    y='value', x='group', \n",
    "                                                    legend=False, title= \"Item count by classes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. sample question: How is the value distribution per class?\n",
    "# item count distribution per class -><- aggregated value distribution per class\n",
    "\n",
    "# aggregated value per classes\n",
    "print(df.groupby(['group'], sort=False).agg({'value': sum}).sort_values('value',ascending=False))\n",
    "\n",
    "\n",
    "# plot it as a histogram\n",
    "df_flat = df.groupby(['group'])['value'].sum().reset_index()\n",
    "print(df_flat.sort_values(['value'], ascending=True).plot(kind='barh',colormap='Pastel2',\n",
    "                                                     y='value', x='group', \n",
    "                                                     legend=False, title= \"Aggregated value per class\"))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution (After text attribute \"transformation\")\n",
    "After doing the necessary preprocessing we can further explore our text data.\n",
    "We can observe the most salient word tokens for each class with using wordclouds as visualization.\n",
    "Each class has its own key-token set, which we can leverage in our classification model.\n",
    "However we can see that there are words which are prominent in more classes e.g. \"service\" in 'Scanner' and 'Server'\n",
    "classes seems frequent -> this can lead to misclassification if we don't handle it (e.g. we can filter out words/terms which\n",
    "are salient in more classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-processed data\n",
    "\n",
    "df = pd.read_csv(\"out/cleaned_data.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordclouds per classes\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=50,\n",
    "        max_font_size=50, \n",
    "        scale=3,\n",
    "        collocations=False,\n",
    "        random_state=1\n",
    "    ).generate(str(data).replace(\"'\",' '))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(8,8))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "df.groupby('group').apply(lambda x: show_wordcloud(x.english_wostopw.tolist(), \n",
    "                                                                         title=f\"{x.name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word frequency table\n",
    "\n",
    "df_most_common3 = df.groupby('group')['english_wostopw'].apply(lambda x: \n",
    "                                                               nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(\n",
    "                                                                                        x))).most_common(3))\n",
    "\n",
    "print(df_most_common3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
